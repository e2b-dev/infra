PREFIX="e2b-" # prefix identifier for all resources

# ------------------------------------------- Provider selection -----------------------------------------------------------
# Select infrastructure provider used by Makefile dispatch: gcp | linux
# Defaults to gcp when unset in Makefile
PROVIDER=gcp

# ------------------------------------------- Provider: GCP -----------------------------------------------------------
# your GCP project ID
GCP_PROJECT_ID=
# GCP region where the resources will be deployed, e.g. us-west1
GCP_REGION=
# GCP zone where the resources will be deployed, e.g. us-west1-a
GCP_ZONE=

# prod, staging, dev
TERRAFORM_ENVIRONMENT=
# GCS bucket name
TERRAFORM_STATE_BUCKET=

# This is for the nomad and consul client (all jobs are running on client)
# e.g. n1-standard-8
CLIENT_MACHINE_TYPE=
# e.g. 1
CLIENT_CLUSTER_SIZE=
# Max number of additional instances if the CPU usage is above 60%, e.g. 2
CLIENT_CLUSTER_SIZE_MAX=

# This is the nomad and consul server (only for scheduling and service discovery)
# eg e2-standard-2
SERVER_MACHINE_TYPE=
# e.g. 3
SERVER_CLUSTER_SIZE=

# eg e2-standard-4
API_MACHINE_TYPE=
# e.g. 1
API_CLUSTER_SIZE=

# eg n1-standard-8
BUILD_MACHINE_TYPE=
# e.g. 1
BUILD_CLUSTER_SIZE=

# e.g. e2-standard-4
CLICKHOUSE_MACHINE_TYPE=
# e.g. 2
CLICKHOUSE_CLUSTER_SIZE=

# your domain or subdomain name, eg great-innovations.dev, e2b.great-innovations.dev
DOMAIN_NAME=

 

FILESTORE_CACHE_ENABLED=
# BASIC_HDD for staging+dev, ZONAL for production
FILESTORE_CACHE_TIER=
# 1024 GB for staging+dev, 1024 GB or 10240 GB for production (10240 is different zonal capacity tier)
FILESTORE_CACHE_CAPACITY_GB=


# ------------------------------------------- Optional block (shared) -----------------------------------------------------------
# your Postgres connection string (shared across providers)
# e.g. postgresql://postgres.<username>:<password>.<host>@<your-domain>:<port>/postgres
POSTGRES_CONNECTION_STRING=
# Managed Redis (default: false)
REDIS_MANAGED=false
# Template bucket name (if you want to use a different bucket for templates then the default one)
TEMPLATE_BUCKET_NAME=
# Hash seed used for generating sandbox access tokens, not needed if you are not using them
SANDBOX_ACCESS_TOKEN_HASH_SEED=abcdefghijklmnopqrstuvwxyz

# Integration tests variables (only for running integration tests locally)
# your domain name, e.g. https://api.great-innovations.dev
TESTS_API_SERVER_URL=
# Host of the orchestrator, e.g. localhost:5008
# If connecting remotely, you might need to bridge the orchestrator connection as it's not publicly available
TESTS_ORCHESTRATOR_HOST=
# Envd proxy, e.g. https://client-proxy.great-innovations.dev
# This can be either session proxy or client proxy, depending on your local setup
TESTS_ENVD_PROXY=
# your sandbox template ID, e.g. base
TESTS_SANDBOX_TEMPLATE_ID=base
# your Team API key
TESTS_E2B_API_KEY=
# your access token
TESTS_E2B_ACCESS_TOKEN=
# your user id
TESTS_SANDBOX_USER_ID=


# ------------------------------------------- Provider: Linux (bare metal) -----------------------------------------------------------
# Datacenter label passed to Consul/Nomad (e.g. "dc1")
DATACENTER=dc1

# Nomad/Consul endpoints and ACL tokens
# Nomad address (HTTP). If running locally on nodes: http://localhost:4646
NOMAD_ADDRESS=http://localhost:4646
# Nomad ACL token (if ACLs are enabled)
NOMAD_ACL_TOKEN=
# Consul ACL token (optional; if set, Consul ACLs are enabled on nodes)
CONSUL_ACL_TOKEN=

# Bare-metal nodes definition (JSON strings)
# Example for SERVERS_JSON: list of Consul/Nomad server nodes
# SERVERS_JSON='[{"host":"10.0.0.10","ssh_user":"ubuntu","ssh_private_key_path":"/home/me/.ssh/id_rsa","node_pool":"servers"}]'
SERVERS_JSON=
# Example for CLIENTS_JSON: list of Nomad client nodes and their node_pool
# CLIENTS_JSON='[{"host":"10.0.0.20","ssh_user":"ubuntu","ssh_private_key_path":"/home/me/.ssh/id_rsa","node_pool":"api"}]'
CLIENTS_JSON=

# Logical node pools used by Nomad jobs (strings). Common values: "api", "builder", "all"
API_NODE_POOL=api
ORCHESTRATOR_NODE_POOL=api
BUILDER_NODE_POOL=builder

# Counts and resources
INGRESS_COUNT=1
API_MACHINE_COUNT=1
API_RESOURCES_CPU_COUNT=1         # Nomad CPU in cores (translated to MHz by jobs)
API_RESOURCES_MEMORY_MB=1024      # Memory in MB

# Port objects for services (JSON strings)
# Format: {"name":"<port-name>","port":<number>,"health_path":"/health"}
# For entries without health_path, omit the key
API_PORT='{"name":"api","port":30000,"health_path":"/health"}'
INGRESS_PORT='{"name":"ingress","port":30001,"health_path":"/health"}'
EDGE_API_PORT='{"name":"edge","port":30002,"path":"/"}'
EDGE_PROXY_PORT='{"name":"proxy","port":30003}'
LOGS_PROXY_PORT='{"name":"logs","port":30004}'
LOKI_SERVICE_PORT='{"name":"loki","port":31000}'
LOGS_HEALTH_PROXY_PORT='{"name":"logs-health","port":8901,"health_path":"/health"}'

# Optional image prefix for Linux provider (registry/namespace)
DOCKER_IMAGE_PREFIX=

# Orchestrator and Template Manager ports
ORCHESTRATOR_PORT=5008
ORCHESTRATOR_PROXY_PORT=5009
TEMPLATE_MANAGER_PORT=5010

# OpenTelemetry Collector endpoint and resources
OTEL_COLLECTOR_GRPC_PORT=4317
OTEL_COLLECTOR_RESOURCES_MEMORY_MB=256
OTEL_COLLECTOR_RESOURCES_CPU_COUNT=1

# Loki resources
LOKI_RESOURCES_MEMORY_MB=1024
LOKI_RESOURCES_CPU_COUNT=1

# Images and artifacts
API_IMAGE=
DB_MIGRATOR_IMAGE=
CLIENT_PROXY_IMAGE=
DOCKER_REVERSE_PROXY_IMAGE=
# Artifact URLs for raw_exec jobs (HTTP/HTTPS). Provide links to orchestrator/template-manager binaries
ORCHESTRATOR_ARTIFACT_URL=
TEMPLATE_MANAGER_ARTIFACT_URL=

ARTIFACT_HTTP_HOST=
ARTIFACT_HTTP_USER=
ARTIFACT_HTTP_DIR=
ARTIFACT_HTTP_SSH_KEY=
ARTIFACT_HTTP_PORT=

# Application secrets and config
API_SECRET=
EDGE_API_SECRET=
API_ADMIN_TOKEN=
SUPABASE_JWT_SECRETS=
POSTHOG_API_KEY=
ANALYTICS_COLLECTOR_HOST=
ANALYTICS_COLLECTOR_API_TOKEN=
LAUNCH_DARKLY_API_KEY=
REDIS_URL=redis.service.consul
REDIS_TLS_CA_BASE64=
REDIS_SECURE_CLUSTER_URL=

# Buckets and caches (used by template-manager/orchestrator)
TEMPLATE_BUCKET_NAME=
BUILD_CACHE_BUCKET_NAME=
SHARED_CHUNK_CACHE_PATH=

# Docker registry remote cache (optional)
DOCKERHUB_REMOTE_REPOSITORY_URL=

# Sandbox/Networking behavior
ENVD_TIMEOUT=30m
ALLOW_SANDBOX_INTERNET=false

# ClickHouse (optional service). Set CLICKHOUSE_SERVER_COUNT>0 in TF if enabling.
CLICKHOUSE_DATABASE=
# Port object for ClickHouse server
# CLICKHOUSE_SERVER_PORT='{"name":"clickhouse","port":9000}'
CLICKHOUSE_SERVER_PORT='{"name":"clickhouse","port":9000}'
CLICKHOUSE_RESOURCES_MEMORY_MB=2048
CLICKHOUSE_RESOURCES_CPU_COUNT=2
CLICKHOUSE_METRICS_PORT=9100

# Hash seed used for generating sandbox access tokens
SANDBOX_ACCESS_TOKEN_HASH_SEED=abcdefghijklmnopqrstuvwxyz
